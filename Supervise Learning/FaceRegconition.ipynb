{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "‚úÖ S·ªë ·∫£nh: 492, nh√£n: {np.str_('Stephanie'), np.str_('Judy'), np.str_('Neri'), np.str_('Valery'), np.str_('Joan'), np.str_('Joshua'), np.str_('Laura'), np.str_('Screenshot 2025-07-23 151535'), np.str_('Stephen'), np.str_('Gavyn'), np.str_('Screenshot 2025-07-23 150534'), np.str_('Screenshot 2025-07-23 151353'), np.str_('Ahmet'), np.str_('Rocco'), np.str_('Screenshot 2025-07-23 151205'), np.str_('Jason'), np.str_('Massoud'), np.str_('Tatiana'), np.str_('Robert'), np.str_('John'), np.str_('Screenshot 2025-07-23 151358'), np.str_('Aaron'), np.str_('Screenshot 2025-07-23 151159'), np.str_('Screenshot 2025-07-23 151155'), np.str_('Screenshot 2025-07-23 151551'), np.str_('Sohail'), np.str_('Xavier'), np.str_('Screenshot 2025-07-23 151624'), np.str_('Screenshot 2025-07-23 150559'), np.str_('Martha'), np.str_('Juan'), np.str_('Lucio'), np.str_('Pamela'), np.str_('Valorie'), np.str_('Michelle'), np.str_('Screenshot 2025-07-23 151051'), np.str_('Lleyton'), np.str_('Screenshot 2025-07-23 150901'), np.str_('Corliss'), np.str_('Screenshot 2025-07-23 150707'), np.str_('Myung'), np.str_('Gary'), np.str_('Donna'), np.str_('Scott'), np.str_('Vassilis'), np.str_('Screenshot 2025-07-23 150854'), np.str_('Hideki'), np.str_('Screenshot 2025-07-23 150849'), np.str_('Emmanuel'), np.str_('Screenshot 2025-07-23 150832'), np.str_('Paul'), np.str_('Felix'), np.str_('Screenshot 2025-07-23 150842'), np.str_('Naomi'), np.str_('Dominic'), np.str_('Screenshot 2025-07-23 150519'), np.str_('Daryl'), np.str_('Flor'), np.str_('Anthony'), np.str_('Screenshot 2025-07-23 150655'), np.str_('Screenshot 2025-07-23 150622'), np.str_('Aitor'), np.str_('Kathleen'), np.str_('Crandall'), np.str_('Cheryl'), np.str_('Michael'), np.str_('Screenshot 2025-07-23 151233'), np.str_('Carla'), np.str_('Sergei'), np.str_('Amanda'), np.str_('Greg'), np.str_('Screenshot 2025-07-23 151711'), np.str_('Screenshot 2025-07-23 151614'), np.str_('Screenshot 2025-07-23 151445'), np.str_('Krishna'), np.str_('Screenshot 2025-07-23 150837'), np.str_('Eddy'), np.str_('Screenshot 2025-07-23 150758'), np.str_('Bo'), np.str_('Screenshot 2025-07-23 150937'), np.str_('Al'), np.str_('Screenshot 2025-07-23 150747'), np.str_('Hank'), np.str_('Teri'), np.str_('Ashraf'), np.str_('Stan'), np.str_('Screenshot 2025-07-23 151409'), np.str_('Roger'), np.str_('Jack'), np.str_('Chuck'), np.str_('Toni'), np.str_('Screenshot 2025-07-23 151620'), np.str_('Screenshot 2025-07-23 150930'), np.str_('Screenshot 2025-07-23 150508'), np.str_('Saeb'), np.str_('Screenshot 2025-07-23 151533'), np.str_('Rohman'), np.str_('Teddy'), np.str_('Dan'), np.str_('Brook'), np.str_('Thomas'), np.str_('Screenshot 2025-07-23 151549'), np.str_('Gene'), np.str_('Ron'), np.str_('Screenshot 2025-07-23 150636'), np.str_('Screenshot 2025-07-23 150522'), np.str_('Patty'), np.str_('Eduard'), np.str_('Amy'), np.str_('Jim'), np.str_('Rose'), np.str_('Lena'), np.str_('Pak'), np.str_('Sergey'), np.str_('Ridley'), np.str_('Donald'), np.str_('sample.jpg'), np.str_('Brian'), np.str_('Screenshot 2025-07-23 150710'), np.str_('Frank'), np.str_('Carlos'), np.str_('Screenshot 2025-07-23 151125'), np.str_('Talisa'), np.str_('Natalie'), np.str_('Andy'), np.str_('Fernando'), np.str_('Andrew'), np.str_('Suh'), np.str_('Screenshot 2025-07-23 150751'), np.str_('Screenshot 2025-07-23 150651'), np.str_('Jana'), np.str_('Iain'), np.str_('Screenshot 2025-07-23 151539'), np.str_('Bill'), np.str_('Andrea'), np.str_('Screenshot 2025-07-23 150612'), np.str_('Screenshot 2025-07-23 151241'), np.str_('Tyler'), np.str_('Screenshot 2025-07-23 151428'), np.str_('Wang'), np.str_('Kwon'), np.str_('Kaio'), np.str_('Marcos'), np.str_('Elias'), np.str_('Jennifer'), np.str_('Jacques'), np.str_('Hillary'), np.str_('James'), np.str_('Screenshot 2025-07-23 151628'), np.str_('Oscar'), np.str_('Celia'), np.str_('Screenshot 2025-07-23 150908'), np.str_('Screenshot 2025-07-23 151114'), np.str_('Keanu'), np.str_('Screenshot 2025-07-23 151151'), np.str_('Dino'), np.str_('Gordon'), np.str_('Nicolas'), np.str_('Screenshot 2025-07-23 151106'), np.str_('Linda'), np.str_('Screenshot 2025-07-23 151142'), np.str_('Screenshot 2025-07-23 151458'), np.str_('Trudi'), np.str_('Jesse'), np.str_('Griffin'), np.str_('Screenshot 2025-07-23 151730'), np.str_('Hikmat'), np.str_('Sharon'), np.str_('Carolina'), np.str_('Alex'), np.str_('Darlene'), np.str_('Lynne'), np.str_('Screenshot 2025-07-23 151506'), np.str_('Bryan'), np.str_('Screenshot 2025-07-23 151716'), np.str_('Dwayne'), np.str_('Romario'), np.str_('Manuel'), np.str_('Ciro'), np.str_('Roy'), np.str_('Mireya'), np.str_('Sivan'), np.str_('Pio'), np.str_('Jaqueline'), np.str_('Angela'), np.str_('Tom'), np.str_('Rob'), np.str_('Jayne'), np.str_('Screenshot 2025-07-23 150912'), np.str_('Eric'), np.str_('Janice'), np.str_('Screenshot 2025-07-23 151721'), np.str_('Screenshot 2025-07-23 150541'), np.str_('Mladen'), np.str_('Screenshot 2025-07-23 150616'), np.str_('Nancy'), np.str_('Emma'), np.str_('Laurence'), np.str_('Jeremy'), np.str_('Marc'), np.str_('Screenshot 2025-07-23 151543'), np.str_('Terry'), np.str_('Screenshot 2025-07-23 150553'), np.str_('SJ'), np.str_('Lee'), np.str_('Alimzhan'), np.str_('Screenshot 2025-07-23 151224'), np.str_('Randall'), np.str_('George'), np.str_('Sada'), np.str_('Hans'), np.str_('Sereyvuth'), np.str_('Zurab'), np.str_('Zeljko'), np.str_('Chris'), np.str_('Screenshot 2025-07-23 150715'), np.str_('Screenshot 2025-07-23 150629'), np.str_('Takeo'), np.str_('Screenshot 2025-07-23 151724'), np.str_('Patsy'), np.str_('Screenshot 2025-07-23 150722'), np.str_('Edward'), np.str_('Bruce'), np.str_('Grant'), np.str_('Clifford'), np.str_('Teresa'), np.str_('Matt'), np.str_('Kristen'), np.str_('Christine'), np.str_('Henry'), np.str_('Christoph'), np.str_('Dale'), np.str_('Mary'), np.str_('Julio'), np.str_('Wayne'), np.str_('Screenshot 2025-07-23 150538'), np.str_('Screenshot 2025-07-23 151642'), np.str_('Steffi'), np.str_('Screenshot 2025-07-23 150529'), np.str_('Timbul'), np.str_('Kevin'), np.str_('Screenshot 2025-07-23 151609'), np.str_('Kate'), np.str_('Screenshot 2025-07-23 151149'), np.str_('Kostya'), np.str_('Hartmut'), np.str_('Mario'), np.str_('Ascencion'), np.str_('Tim'), np.str_('Colleen'), np.str_('Mel'), np.str_('Screenshot 2025-07-23 151228'), np.str_('Screenshot 2025-07-23 151713'), np.str_('Barry'), np.str_('Sebastian'), np.str_('Madeleine'), np.str_('Tyron'), np.str_('Screenshot 2025-07-23 151449'), np.str_('William'), np.str_('Screenshot 2025-07-23 151718'), np.str_('Lloyd'), np.str_('Screenshot 2025-07-23 151053'), np.str_('Screenshot 2025-07-23 151211'), np.str_('Screenshot 2025-07-23 151607'), np.str_('Jimmy'), np.str_('Mike'), np.str_('Mark'), np.str_('Tommy'), np.str_('Screenshot 2025-07-23 150602'), np.str_('Craig'), np.str_('Screenshot 2025-07-23 151216'), np.str_('Shavon'), np.str_('Screenshot 2025-07-23 151611'), np.str_('Screenshot 2025-07-23 151237'), np.str_('Jerry'), np.str_('Julian'), np.str_('Bing'), np.str_('Naoto'), np.str_('Stella'), np.str_('Steven'), np.str_('Screenshot 2025-07-23 150515'), np.str_('Screenshot 2025-07-23 151635'), np.str_('Norah'), np.str_('Screenshot 2025-07-23 151640'), np.str_('Maureen'), np.str_('Sidney'), np.str_('Screenshot 2025-07-23 151726'), np.str_('Rick'), np.str_('Steny'), np.str_('Billy'), np.str_('Screenshot 2025-07-23 150626'), np.str_('Peter'), np.str_('Screenshot 2025-07-23 150803'), np.str_('Francis'), np.str_('Bob'), np.str_('Screenshot 2025-07-23 151059'), np.str_('Gen'), np.str_('Screenshot 2025-07-23 150845'), np.str_('Chip'), np.str_('Charles'), np.str_('Tiffany'), np.str_('Sheryl'), np.str_('Cristian'), np.str_('Maria'), np.str_('Gerard'), np.str_('Santiago'), np.str_('Osrat'), np.str_('Screenshot 2025-07-23 150633'), np.str_('Gabrielle'), np.str_('Mira'), np.str_('Screenshot 2025-07-23 151048'), np.str_('Steve'), np.str_('Screenshot 2025-07-23 151436'), np.str_('Dick'), np.str_('Erika'), np.str_('Marcella'), np.str_('Wanda'), np.str_('Adam'), np.str_('Ajit'), np.str_('Screenshot 2025-07-23 150512'), np.str_('Screenshot 2025-07-23 150548'), np.str_('Jonathan'), np.str_('Screenshot 2025-07-23 151346'), np.str_('Inocencio'), np.str_('David'), np.str_('Betty'), np.str_('Monica'), np.str_('Javier'), np.str_('Ernie'), np.str_('Shane'), np.str_('Screenshot 2025-07-23 150701'), np.str_('Olivier'), np.str_('Screenshot 2025-07-23 151632'), np.str_('Ted'), np.str_('Ronald'), np.str_('Tara')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ train & l∆∞u model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0)\n",
    "\n",
    "def load_cropface_embeddings(folder):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        if not filename.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        label = filename.split('_')[0]  # L·∫•y t√™n tr∆∞·ªõc d·∫•u _\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, (112, 112))  # ƒë·∫£m b·∫£o ƒë√∫ng input size\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Tr√≠ch embedding t·ª´ ·∫£nh ƒë√£ crop\n",
    "        embedding = app.models['recognition'].get_feat(img).flatten()\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# B∆∞·ªõc 1: Load ·∫£nh v√† tr√≠ch embedding\n",
    "X, y = load_cropface_embeddings(\"facecrop\")\n",
    "print(f\"‚úÖ S·ªë ·∫£nh: {len(X)}, nh√£n: {set(y)}\")\n",
    "\n",
    "if len(X) < 2 or len(set(y)) < 2:\n",
    "    raise ValueError(\"‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ train SVM. C·∫ßn √≠t nh·∫•t 2 ·∫£nh t·ª´ 2 nh√£n kh√°c nhau.\")\n",
    "\n",
    "# B∆∞·ªõc 2: Encode nh√£n\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# B∆∞·ªõc 3: Train SVM\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(X, y_encoded)\n",
    "\n",
    "# B∆∞·ªõc 4: L∆∞u model\n",
    "joblib.dump(clf, \"svm_face_model.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ train & l∆∞u model.\")\n",
    "\n",
    "# OMG IT WORKEDDDDDDDD AJNWDJNAWDJKNAWJDNKAWKJDAWNJDAWKJNDAWJKDJAKWDANWKJDJKNAWKDNJAWNJK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244295cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_16130/1655882791.py\", line 5, in <module>\n",
      "    from insightface.app import FaceAnalysis\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/insightface/__init__.py\", line 18, in <module>\n",
      "    from . import app\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/insightface/app/__init__.py\", line 2, in <module>\n",
      "    from .mask_renderer import *\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/insightface/app/mask_renderer.py\", line 4, in <module>\n",
      "    import albumentations as A\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py\", line 24, in <module>\n",
      "    from .pytorch import *\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/albumentations/pytorch/__init__.py\", line 1, in <module>\n",
      "    from .transforms import *\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/albumentations/pytorch/transforms.py\", line 15, in <module>\n",
      "    import torch\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "‚úÖ Nh·∫≠n di·ªán: Ev (ƒë·ªô t·ª± tin: 93.59%)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Load SVM model v√† LabelEncoder\n",
    "clf = joblib.load(\"svm_face_model.pkl\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Load InsightFace\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0)\n",
    "\n",
    "def predict_face(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}\")\n",
    "\n",
    "    img = cv2.resize(img, (112, 112))  # resize cho ƒë√∫ng k√≠ch th∆∞·ªõc\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # T·∫°o gi·∫£ ƒë·ªãnh face object v√¨ ·∫£nh ƒë√£ crop r·ªìi\n",
    "    face = type('Face', (object,), {})()\n",
    "    face.kps = None  # Kh√¥ng c√≥ landmark\n",
    "    embedding = app.models['recognition'].get_feat(img).flatten()\n",
    "\n",
    "    # Predict\n",
    "    pred = clf.predict([embedding])\n",
    "    prob = clf.predict_proba([embedding])\n",
    "\n",
    "    label = label_encoder.inverse_transform(pred)[0]\n",
    "    confidence = prob[0][pred[0]]\n",
    "\n",
    "    print(f\"‚úÖ Nh·∫≠n di·ªán: {label} (ƒë·ªô t·ª± tin: {confidence:.2%})\")\n",
    "\n",
    "# Test m·ªôt ·∫£nh\n",
    "predict_face(\"datasets/testcrop/test.png\")  # ƒê·ªïi path n·∫øu c·∫ßn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda3510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Load SVM v√† label encoder\n",
    "clf = joblib.load(\"svm_face_model.pkl\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Load InsightFace\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0)\n",
    "\n",
    "# Kh·ªüi ƒë·ªông webcam (n·∫øu c√≥ webcam th·∫≠t, ho·∫∑c ƒëang d√πng video m√¥ ph·ªèng)\n",
    "cap = cv2.VideoCapture(\"output.avi\")  # ƒê·ªïi th√†nh ƒë∆∞·ªùng d·∫´n video n·∫øu c·∫ßn\n",
    "\n",
    "frame_count = 0  # ƒê·∫øm frame ƒë·ªÉ debug n·∫øu c·∫ßn\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Chuy·ªÉn sang RGB ƒë·ªÉ insightface d√πng\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face\n",
    "    faces = app.get(img_rgb)\n",
    "\n",
    "    for face in faces:\n",
    "        if face.embedding is None:\n",
    "            continue  # Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c\n",
    "\n",
    "        embedding = face.embedding\n",
    "        pred = clf.predict([embedding])[0]\n",
    "        prob = clf.predict_proba([embedding])[0][pred]\n",
    "\n",
    "        label = label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "        # V·∫Ω bounding box + label\n",
    "        x1, y1, x2, y2 = map(int, face.bbox)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label} ({prob:.2%})\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # ‚úÖ Thay v√¨ imshow, ta l∆∞u ·∫£nh ra file (ho·∫∑c log ra console)\n",
    "    output_path = f\"output/frame_{frame_count}.jpg\"\n",
    "    cv2.imwrite(output_path, frame)\n",
    "    print(f\"[INFO] Frame {frame_count} saved to {output_path}\")\n",
    "    frame_count += 1\n",
    "\n",
    "    # Ch·∫°y th·ª≠ 50 frame r·ªìi d·ª´ng (tr√°nh ch·∫°y m√£i trong Docker)\n",
    "    if frame_count >= 50:\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c53a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current dir: /workspace/Supervise Learning\n",
      "üìÇ Files in dir: ['.gradio', '.ipynb_checkpoints', 'CNN.ipynb', 'datasets', 'facecrop', 'FaceRegconition.ipynb', 'image', 'label_encoder.pkl', 'lfw_funneled', 'ML.ipynb', 'mnist_cnn_model.h5', 'PredictbyType.ipynb', 'PredictbyValue.ipynb', 'svm_face_model.pkl', 'Test Drawing Program.ipynb', 'yolov8n-face.pt']\n",
      "üé• Opening video: output.avi\n",
      "‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c video. C√≥ th·ªÉ do:\n",
      "  1. Video kh√¥ng t·ªìn t·∫°i\n",
      "  2. OpenCV kh√¥ng h·ªó tr·ª£ codec (thi·∫øu FFMPEG)\n",
      "  3. File b·ªã l·ªói\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "video_path = \"output.avi\"\n",
    "print(\"üìÅ Current dir:\", os.getcwd())\n",
    "print(\"üìÇ Files in dir:\", os.listdir(\".\"))\n",
    "print(\"üé• Opening video:\", video_path)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c video. C√≥ th·ªÉ do:\")\n",
    "    print(\"  1. Video kh√¥ng t·ªìn t·∫°i\")\n",
    "    print(\"  2. OpenCV kh√¥ng h·ªó tr·ª£ codec (thi·∫øu FFMPEG)\")\n",
    "    print(\"  3. File b·ªã l·ªói\")\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
